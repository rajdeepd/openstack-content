<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>OpenStack Installation</title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF" width="980px"><div class="book"><div class="titlepage"><div><div><h1 class="title"><a name="example-changeme"></a>OpenStack Installation</h1></div><div><div class="author"><h3 class="author"><span class="firstname"></span> <span class="surname"></span></h3></div></div><div><p class="releaseinfo">Version 1.0</p></div><div><p class="copyright">Copyright © 2013 Rajdeep Dua, Raja Reddy</p></div><div><p class="pubdate">2013-07-01</p></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl class="toc"><dt><span class="chapter"><a href="#basic-install_procs">1. Install</a></span></dt><dd><dl><dt><span class="section"><a href="#basic-install_requirements">Requirements</a></span></dt><dt><span class="section"><a href="#basic-install_controller">Controller Node</a></span></dt><dd><dl><dt><span class="section"><a href="#basic-install_controller-intro">Introduction</a></span></dt><dt><span class="section"><a href="#basic-install_controller-common">Common services</a></span></dt><dt><span class="section"><a href="#basic-install_controller-keystone">OpenStack Identity Service</a></span></dt><dt><span class="section"><a href="#basic-install_controller-glance">OpenStack Image Service</a></span></dt><dt><span class="section"><a href="#basic-install_controller-nova">OpenStack Compute (Cloud Controller services)</a></span></dt><dt><span class="section"><a href="#basic-install_controller-cinder">OpenStack Block Storage</a></span></dt><dt><span class="section"><a href="#basic-install_controller-quantum">OpenStack Network Service (Cloud Controller)</a></span></dt><dt><span class="section"><a href="#basic-install_controller-dashboard">OpenStack Dashboard</a></span></dt></dl></dd><dt><span class="section"><a href="#basic-install_network">Network Node</a></span></dt><dd><dl><dt><span class="section"><a href="#basic-install_network-intro">Introduction</a></span></dt><dt><span class="section"><a href="#basic-network-common">Common services</a></span></dt><dt><span class="section"><a href="#basic-install_network-services">OpenStack Networking (Network Controller)</a></span></dt><dt><span class="section"><a href="#basic-install_network-operating">Virtual Networking</a></span></dt></dl></dd><dt><span class="section"><a href="#basic-install_compute">Compute Node</a></span></dt><dd><dl><dt><span class="section"><a href="#basic-install_compute-intro">Introduction</a></span></dt><dt><span class="section"><a href="#basic-install_compute-common">Common services</a></span></dt><dt><span class="section"><a href="#basic-install_compute-nova">OpenStack Compute (Compute Node services)</a></span></dt><dt><span class="section"><a href="#basic-install_compute-quantum">OpenStack Networking (Compute Node)</a></span></dt></dl></dd></dl></dd></dl></div><div class="list-of-tables"><p><b>List of Tables</b></p><dl><dt>1.1. <a href="#idp8608">Architecture and node information</a></dt></dl></div><div class="chapter"><div class="titlepage"><div><div><h1 class="title"><a name="basic-install_procs"></a>Chapter 1. Install</h1></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl class="toc"><dt><span class="section"><a href="#basic-install_requirements">Requirements</a></span></dt><dt><span class="section"><a href="#basic-install_controller">Controller Node</a></span></dt><dd><dl><dt><span class="section"><a href="#basic-install_controller-intro">Introduction</a></span></dt><dt><span class="section"><a href="#basic-install_controller-common">Common services</a></span></dt><dt><span class="section"><a href="#basic-install_controller-keystone">OpenStack Identity Service</a></span></dt><dt><span class="section"><a href="#basic-install_controller-glance">OpenStack Image Service</a></span></dt><dt><span class="section"><a href="#basic-install_controller-nova">OpenStack Compute (Cloud Controller services)</a></span></dt><dt><span class="section"><a href="#basic-install_controller-cinder">OpenStack Block Storage</a></span></dt><dt><span class="section"><a href="#basic-install_controller-quantum">OpenStack Network Service (Cloud Controller)</a></span></dt><dt><span class="section"><a href="#basic-install_controller-dashboard">OpenStack Dashboard</a></span></dt></dl></dd><dt><span class="section"><a href="#basic-install_network">Network Node</a></span></dt><dd><dl><dt><span class="section"><a href="#basic-install_network-intro">Introduction</a></span></dt><dt><span class="section"><a href="#basic-network-common">Common services</a></span></dt><dt><span class="section"><a href="#basic-install_network-services">OpenStack Networking (Network Controller)</a></span></dt><dt><span class="section"><a href="#basic-install_network-operating">Virtual Networking</a></span></dt></dl></dd><dt><span class="section"><a href="#basic-install_compute">Compute Node</a></span></dt><dd><dl><dt><span class="section"><a href="#basic-install_compute-intro">Introduction</a></span></dt><dt><span class="section"><a href="#basic-install_compute-common">Common services</a></span></dt><dt><span class="section"><a href="#basic-install_compute-nova">OpenStack Compute (Compute Node services)</a></span></dt><dt><span class="section"><a href="#basic-install_compute-quantum">OpenStack Networking (Compute Node)</a></span></dt></dl></dd></dl></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="basic-install_requirements"></a>Requirements</h2></div></div></div><p>You need at least three machines, virtual or physical, with
            <span class="phrase">Ubuntu 12.04 LTS</span>
        <span class="phrase">Fedora 18</span>
        installed.</p><div class="table"><a name="idp8608"></a><p class="title"><b>Table 1.1. Architecture and node information</b></p><div class="table-contents"><table rules="all" frame="border" width="100%"><thead><tr>
                <th rowspan="2"></th>
                <th colspan="3" align="center"><p>Nodes</p></th>
            </tr><tr>
                <th><p>controller</p></th>
                <th><p>network</p></th>
                <th><p>compute</p>
                </th>
            </tr></thead><tbody><tr>
                <th><p>Hostname</p></th>
                <td><p>cloud</p></td>
                <td><p>network</p></td>
                <td><p>c01</p>
                </td>
            </tr><tr>
                <th>
                    <p>Services</p></th>
                <td><p>MySQL, <span class="phrase">RabbitMQ</span>
                        <span class="phrase">Qpid</span>, Nova,
                        Cinder, Glance, Keystone, Quantum</p></td>
                <td><p>Quantum-L3-agent, Quantum-DHCP-agent,
                        Quantum Agent with Open-vSwitch</p></td>
                <td><p>nova-compute, KVM, nova-api, Quantum Agent
                        with Open-vSwitch</p>
                </td>
            </tr><tr>
                <th>
                    <p>Minimum number of disks</p></th>
                <td><p>2</p></td>
                <td><p>1</p></td>
                <td><p>1</p>
                </td>
            </tr><tr>
                <th>
                    <p>External</p></th>
                <td><p>10.0.0.10/24</p></td>
                <td><p>10.0.0.9/24</p></td>
                <td><p>-</p>
                </td>
            </tr><tr>
                <th>
                    <p>Internal network</p></th>
                <td><p>10.10.10.10/24</p></td>
                <td><p>10.10.10.9/24</p></td>
                <td><p>10.10.10.11/24</p>
                </td>
            </tr><tr>
                <th><p><span class="bold"><strong>Total number of NIC</strong></span></p></th>
                <td><p><span class="bold"><strong>2</strong></span></p></td>
                <td><p><span class="bold"><strong>2</strong></span></p></td>
                <td><p><span class="bold"><strong>1</strong></span></p>
                </td>
            </tr></tbody></table></div></div><br class="table-break"></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="basic-install_controller"></a>Controller Node</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_controller-intro"></a>Introduction</h3></div></div></div><p>The Controller node will provide :
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Databases (with MySQL)</p></li><li class="listitem"><p>Queues <span class="phrase">(with Qpid)</span>
                            <span class="phrase">(with RabbitMQ)</span>
                            </p></li><li class="listitem"><p>Keystone</p></li><li class="listitem"><p>Glance</p></li><li class="listitem"><p>Nova (without nova-compute)</p></li><li class="listitem"><p>Cinder</p></li><li class="listitem"><p>Quantum Server (with Open-vSwitch plugin)</p></li><li class="listitem"><p>Dashboard (with Horizon)</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_controller-common"></a>Common services</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="controller-os"></a>Operating System</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install <span class="phrase">Fedora 18</span>
                    <span class="phrase">Ubuntu 12.04 or 13.04</span>. 
                    The exact installation procedure is outside the scope of 
                    this document, but please note the following 
                    configurations:
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>
                                Time zone: <span class="bold"><strong>UTC</strong></span>
                            </p></li><li class="listitem"><p>
                                Hostname: <span class="bold"><strong>cloud</strong></span>
                            </p></li><li class="listitem"><p>
                                Packages: <span class="bold"><strong>OpenSSH-Server</strong></span>, 
                                <span class="bold"><strong>wget</strong></span>
                            </p></li></ul></div><p>
                </p><p>
                    Once installation has finished, the server will reboot.
                </p></li><li class="listitem"><p>Since the default OpenStack release in 
                    Ubuntu 12.04 LTS is older, we are going to use the Ubuntu 
                    Cloud Archive for Grizzly:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install ubuntu-cloud-keyring</code></strong></pre><p>
                    Edit <span class="bold"><strong>/etc/apt/sources.list.d/cloud-archive.list</strong></span>:
                        </p><pre class="programlisting">deb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/grizzly main</pre><p>
                    Upgrade the system (and reboot if you need):
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get update &amp;&amp; apt-get dist-upgrade</code></strong></pre><p>
                </p><p>Use the Fedora
                    repositories for Grizzly:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>wget http://repos.fedorapeople.org/repos/openstack/openstack-grizzly/fedora-openstack-grizzly.repo</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>mv fedora-openstack-grizzly.repo /etc/yum.repos.d/</code></strong></pre><p>
                </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>For CentOS, use http://repos.fedorapeople.org/repos/openstack/openstack-grizzly/epel-openstack-grizzly.repo.</p></div></li><li class="listitem"><p>Configure the network:</p><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <span class="bold"><strong>/etc/network/interfaces</strong></span>:
                                </p><pre class="programlisting"># Internal Network
auto eth0
    iface eth0 inet static
    address 10.10.10.10
    netmask 255.255.255.0

# External Network
    auto eth1
    iface eth1 inet static
    address 10.0.0.10
    netmask 255.255.255.0
    gateway 10.0.0.1
    dns-nameservers 8.8.8.8</pre><p>
                                
                            </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysctl.conf</strong></span>:
                                </p><pre class="programlisting">net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0</pre><p>
                                Then, restart the network service:
                                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service networking restart</code></strong></pre><p>
                            </p></li></ul></div><p>
                </p><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Set up old ethernet nic device names:</p><p>
                                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>sed -i 's/# GOTO="netdevicename_end"/GOTO="netdevicename_end"/g' /lib/udev/rules.d/71-biosdevname.rules</code></strong></pre><p>
                            </p></li><li class="listitem"><p>Disable NetworkManager and enable the network service
                                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service NetworkManager stop</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service network start</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig NetworkManager off</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig network on</code></strong></pre><p>
                            </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysconfig/network-scripts/ifcfg-eth0</strong></span>:
                                </p><pre class="programlisting"># Internal Network
DEVICE=eth0
TYPE=Ethernet
BOOTPROTO=static
IPADDR=10.10.10.10
NETMASK=255.255.255.0
DEFROUTE=yes
ONBOOT=yes
</pre><p>
                                
                            </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysconf/network-scripts/ifcfg-eth1</strong></span>:
                                </p><pre class="programlisting"># External Network
DEVICE=eth1
TYPE=Ethernet
BOOTPROTO=static
IPADDR=10.0.0.10
NETMASK=255.255.255.0
GATEWAY=10.0.0.1
DNS=8.8.8.8
DEFROUTE=yes
ONBOOT=yes
</pre><p>
                                
                            </p></li><li class="listitem"><p>
                            Reboot
                        </p></li></ul></div><p>
                </p><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit the <span class="bold"><strong>/etc/hosts</strong></span> file and add
                                <span class="bold"><strong>cloud</strong></span>, 
                                <span class="bold"><strong>network</strong></span>, and 
                                <span class="bold"><strong>c01</strong></span> hostnames with correct IP.
                                </p><pre class="programlisting">127.0.0.1       localhost
10.10.10.10     cloud
10.10.10.9      network
10.10.10.11     c01</pre><p>
                                    
                                </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
                                        While manually specifying host entries is acceptable for a simple or testing environment, it is highly recommended to
                                        use proper DNS entries, or at a minimum a configuration management system such as Puppet, to maintain your IP to
                                        host mappings.
                                    </p></div><p>
                            </p></li></ul></div><p>
                </p></li><li class="listitem"><p>Install NTP. NTP will ensure that the server has the correct time. This is important because if an OpenStack server's time is
                    not correct, it will be removed from the rest of the cloud.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p> 
                                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install ntp</code></strong></pre><p>
                                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install ntp</code></strong></pre><p>
                            </p></li></ul></div><p>
                </p></li></ol></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="controller-mysql"></a>MySQL Database Service</h4></div></div></div><p>The various OpenStack components store
            persistent data in a relational database. MySQL is the
            most popular choice. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the packages: </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install python-mysqldb mysql-server</code></strong></pre><p>
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install mysql mysql-server MySQL-python</code></strong></pre><p>
                        </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p><code class="literal">apt-get</code> will
                                prompt you to set the MySQL root
                                password.</p></div><p>
                    </p></li><li class="listitem"><p>By default, MySQL will only accept
                        connections from localhost. This needs changed
                        so that the compute nodes can access the
                        OpenStack Networking service. Database
                        requests for the OpenStack Compute service are
                        proxied through the
                            <code class="literal">nova-conductor</code> service. </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mysql/my.cnf</code></strong></pre><p>
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>sed -i 's/127.0.0.1/0.0.0.0/g' /etc/my.cnf</code></strong></pre></li><li class="listitem"><p>Restart the service: </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service mysql restart</code></strong></pre><p>
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>systemctl start mysqld.service</code></strong></pre><p>
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>chkconfig mysqld on</code></strong></pre></li><li class="listitem"><p>The various databases that the OpenStack
                        services require need to be created. Additionally,
                        MySQL accounts to access those databases need
			to be created:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>mysql -u root -p &lt;&lt;EOF
CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
IDENTIFIED BY 'password';
CREATE DATABASE cinder;
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \
IDENTIFIED BY 'password';
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
IDENTIFIED BY 'password';
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
IDENTIFIED BY 'password';
CREATE DATABASE quantum;
GRANT ALL PRIVILEGES ON quantum.* TO 'quantum'@'localhost' \
IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON quantum.* TO 'quantum'@'10.10.10.9' \
IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON quantum.* TO 'quantum'@'10.10.10.11' \
IDENTIFIED BY 'password';
FLUSH PRIVILEGES;
EOF</code></strong></pre></li></ol></div><p>
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="controller-rabbitmq"></a> <span class="phrase">Qpid</span>
                    <span class="phrase">RabbitMQ</span> Messaging Service</h4></div></div></div><p>The OpenStack components also communicate through a queuing service. For example, the Cloud Controller
                    places a request to launch an instance on the queue. The Compute Node then picks this request up
                    and launches the instance. OpenStack can work with several different queuing services. 
                </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the packages:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install rabbitmq-server</code></strong></pre><p>
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install qpid-cpp-server</code></strong></pre><p>
                    </p></li><li class="listitem"><p>Change the default password:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>rabbitmqctl change_password guest password</code></strong></pre><p>
                            </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>In addition to choosing another password in a production environment, you should also
                                    disable the guest account and use a proper RabbitMQ account. Please see the RabbitMQ
                                    documentation for further details.
                                </p></div><p>
                        </p><p>Enable authentication:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>echo auth=1 &gt;&gt; /etc/qpidd.conf</code></strong></pre><p>                        
                        </p></li><li class="listitem"><p>Enable the messaging service:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>chkconfig qpidd on</code></strong></pre></li><li class="listitem"><p>Start the messaging service:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service qpidd start</code></strong></pre></li></ol></div><p>
            </p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_controller-keystone"></a>OpenStack Identity Service</h3></div></div></div><p>The OpenStack Identity Service provides the cloud environment with an authentication and authorization system. In this system,
        users are a part of one or more projects. In each of these projects, they hold a specific role. 
        </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the packages:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install keystone python-keystone python-keystoneclient</code></strong></pre><p>
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openstack-keystone python-keystone python-keystoneclient</code></strong></pre></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/keystone/keystone.conf</strong></span>:
                    </p><pre class="programlisting">[DEFAULT]
admin_token = password
debug = True
verbose = True

[sql]
connection = mysql://keystone:password@localhost/keystone</pre></li><li class="listitem"><p>Create the ssl keys:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>keystone-manage pki_setup</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chown -R keystone:keystone /etc/keystone/*</code></strong></pre></li><li class="listitem"><p>Restart Keystone and create the tables in the database:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service keystone restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>keystone-manage db_sync</code></strong>
                    </pre><p>
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openstack-keystone restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>openstack-db --init --service keystone </code></strong></pre><p>
                    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Check the <code class="literal">/var/log/keystone/keystone.log</code> file for errors that would
                            prevent the Identity Service from successfully starting.</p></div><p>                    
                </p></li><li class="listitem"><p>Create an <code class="literal">openrc</code> File</p><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Create a file called <span class="bold"><strong>~/openrc</strong></span>. This
                                file contains the OpenStack admin credentials that will be used when interacting with the OpenStack 
                                environment on the command line.
                                </p><pre class="programlisting">export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=password
export OS_AUTH_URL="http://localhost:5000/v2.0/"
export SERVICE_ENDPOINT="http://localhost:35357/v2.0"
export SERVICE_TOKEN=password</pre></li></ul></div><p>
                </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Source the credentials into your environment:
                            </p><pre class="screen"><strong class="userinput"><code>source ~/openrc</code></strong></pre></li><li class="listitem"><p>
                            Configure the Bash shell to load these credentials upon each login:
                            </p><pre class="screen"><strong class="userinput"><code>echo "source ~/openrc" &gt;&gt; ~/.bashrc</code></strong></pre><p>
                        </p></li></ul></div></li><li class="listitem"><p>The following bash script will populate Keystone with some initial data:
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Projects: admin and services</p></li><li class="listitem"><p>Roles: admin, Member</p></li><li class="listitem"><p>Users: admin, demo, nova, glance, quantum, and cinder</p></li><li class="listitem"><p>Services: compute, volume, image, identity, ec2, and network</p></li></ul></div><p>
                    </p><pre class="programlisting">#!/bin/bash

# Modify these variables as needed
ADMIN_PASSWORD=${ADMIN_PASSWORD:-password}
SERVICE_PASSWORD=${SERVICE_PASSWORD:-$ADMIN_PASSWORD}
DEMO_PASSWORD=${DEMO_PASSWORD:-$ADMIN_PASSWORD}
export OS_SERVICE_TOKEN="password"
export OS_SERVICE_ENDPOINT="http://localhost:35357/v2.0"
SERVICE_TENANT_NAME=${SERVICE_TENANT_NAME:-service}
#
MYSQL_USER=keystone
MYSQL_DATABASE=keystone
MYSQL_HOST=localhost
MYSQL_PASSWORD=password
#
KEYSTONE_REGION=RegionOne
KEYSTONE_HOST=10.10.10.10

# Shortcut function to get a newly generated ID
function get_field() {
    while read data; do
        if [ "$1" -lt 0 ]; then
            field="(\$(NF$1))"
        else
            field="\$$(($1 + 1))"
        fi
        echo "$data" | awk -F'[ \t]*\\|[ \t]*' "{print $field}"
    done
}

# Tenants
ADMIN_TENANT=$(keystone tenant-create --name=admin | grep " id " | get_field 2)
DEMO_TENANT=$(keystone tenant-create --name=demo | grep " id " | get_field 2)
SERVICE_TENANT=$(keystone tenant-create --name=$SERVICE_TENANT_NAME | grep " id " | get_field 2)

# Users
ADMIN_USER=$(keystone user-create --name=admin --pass="$ADMIN_PASSWORD" --email=admin@domain.com | grep " id " | get_field 2)
DEMO_USER=$(keystone user-create --name=demo --pass="$DEMO_PASSWORD" --email=demo@domain.com --tenant-id=$DEMO_TENANT | grep " id " | get_field 2)
NOVA_USER=$(keystone user-create --name=nova --pass="$SERVICE_PASSWORD" --tenant-id $SERVICE_TENANT --email=nova@domain.com | grep " id " | get_field 2)
GLANCE_USER=$(keystone user-create --name=glance --pass="$SERVICE_PASSWORD" --tenant-id $SERVICE_TENANT --email=glance@domain.com | grep " id " | get_field 2)
QUANTUM_USER=$(keystone user-create --name=quantum --pass="$SERVICE_PASSWORD" --tenant-id $SERVICE_TENANT --email=quantum@domain.com | grep " id " | get_field 2)
CINDER_USER=$(keystone user-create --name=cinder --pass="$SERVICE_PASSWORD" --tenant-id $SERVICE_TENANT --email=cinder@domain.com | grep " id " | get_field 2)

# Roles
ADMIN_ROLE=$(keystone role-create --name=admin | grep " id " | get_field 2)
MEMBER_ROLE=$(keystone role-create --name=Member | grep " id " | get_field 2)

# Add Roles to Users in Tenants
keystone user-role-add --user-id $ADMIN_USER --role-id $ADMIN_ROLE --tenant-id $ADMIN_TENANT
keystone user-role-add --tenant-id $SERVICE_TENANT --user-id $NOVA_USER --role-id $ADMIN_ROLE
keystone user-role-add --tenant-id $SERVICE_TENANT --user-id $GLANCE_USER --role-id $ADMIN_ROLE
keystone user-role-add --tenant-id $SERVICE_TENANT --user-id $QUANTUM_USER --role-id $ADMIN_ROLE
keystone user-role-add --tenant-id $SERVICE_TENANT --user-id $CINDER_USER --role-id $ADMIN_ROLE
keystone user-role-add --tenant-id $DEMO_TENANT --user-id $DEMO_USER --role-id $MEMBER_ROLE

# Create services
COMPUTE_SERVICE=$(keystone service-create --name nova --type compute --description 'OpenStack Compute Service' | grep " id " | get_field 2)
VOLUME_SERVICE=$(keystone service-create --name cinder --type volume --description 'OpenStack Volume Service' | grep " id " | get_field 2)
IMAGE_SERVICE=$(keystone service-create --name glance --type image --description 'OpenStack Image Service' | grep " id " | get_field 2)
IDENTITY_SERVICE=$(keystone service-create --name keystone --type identity --description 'OpenStack Identity' | grep " id " | get_field 2)
EC2_SERVICE=$(keystone service-create --name ec2 --type ec2 --description 'OpenStack EC2 service' | grep " id " | get_field 2)
NETWORK_SERVICE=$(keystone service-create --name quantum --type network --description 'OpenStack Networking service' | grep " id " | get_field 2)

# Create endpoints
keystone endpoint-create --region $KEYSTONE_REGION --service-id $COMPUTE_SERVICE --publicurl 'http://'"$KEYSTONE_HOST"':8774/v2/$(tenant_id)s' --adminurl 'http://'"$KEYSTONE_HOST"':8774/v2/$(tenant_id)s' --internalurl 'http://'"$KEYSTONE_HOST"':8774/v2/$(tenant_id)s'
keystone endpoint-create --region $KEYSTONE_REGION --service-id $VOLUME_SERVICE --publicurl 'http://'"$KEYSTONE_HOST"':8776/v1/$(tenant_id)s' --adminurl 'http://'"$KEYSTONE_HOST"':8776/v1/$(tenant_id)s' --internalurl 'http://'"$KEYSTONE_HOST"':8776/v1/$(tenant_id)s'
keystone endpoint-create --region $KEYSTONE_REGION --service-id $IMAGE_SERVICE --publicurl 'http://'"$KEYSTONE_HOST"':9292' --adminurl 'http://'"$KEYSTONE_HOST"':9292' --internalurl 'http://'"$KEYSTONE_HOST"':9292'
keystone endpoint-create --region $KEYSTONE_REGION --service-id $IDENTITY_SERVICE --publicurl 'http://'"$KEYSTONE_HOST"':5000/v2.0' --adminurl 'http://'"$KEYSTONE_HOST"':35357/v2.0' --internalurl 'http://'"$KEYSTONE_HOST"':5000/v2.0'
keystone endpoint-create --region $KEYSTONE_REGION --service-id $EC2_SERVICE --publicurl 'http://'"$KEYSTONE_HOST"':8773/services/Cloud' --adminurl 'http://'"$KEYSTONE_HOST"':8773/services/Admin' --internalurl 'http://'"$KEYSTONE_HOST"':8773/services/Cloud'
keystone endpoint-create --region $KEYSTONE_REGION --service-id $NETWORK_SERVICE --publicurl 'http://'"$KEYSTONE_HOST"':9696/' --adminurl 'http://'"$KEYSTONE_HOST"':9696/' --internalurl 'http://'"$KEYSTONE_HOST"':9696/'
                    </pre><p>                    
                </p></li></ol></div><p>
        </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
                If you make a mistake during this guide, you can reset the Keystone database by performing the following steps:
                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>mysql -u root -p keystone -e "drop database keystone"</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>mysql -u root -p keystone -e "create database keystone"</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>mysql -u root -p keystone -e "grant all privileges on keystone.* TO 'keystone'@'localhost' identified by 'password'"</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>keystone-manage db_sync</code></strong></pre><p>
                And finally, re-run the above bash script.
            </p></div><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_controller-glance"></a>OpenStack Image Service</h3></div></div></div><p>The Image Service provides the cloud environment with a catalog of virtual machine "templates". These templates are used as the basis
        of instances. For example, if the catalog contains an image for an Ubuntu 12.04 distribution, the users of the cloud will be able
        to launch Ubuntu 12.04 instances.
                </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the Glance packages:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install glance glance-api glance-registry python-glanceclient glance-common</code></strong></pre><p>
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openstack-glance openstack-glance-api openstack-glance-registry python-glanceclient glance-common</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Configure Glance:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Glance consists of two services: <code class="literal">glance-api</code> and <code class="literal">glance-registry</code>.
                                    For a basic setup, they are configured identically, however, be aware that they provide two distinct services.</p><p>Edit <span class="bold"><strong>/etc/glance/glance-api.conf</strong></span> and <span class="bold"><strong>/etc/glance/glance-registry.conf</strong></span>:
                                    </p><pre class="programlisting">[DEFAULT]
sql_connection = mysql://glance:password@localhost/glance
[keystone_authtoken]
admin_tenant_name = service
admin_user = glance
admin_password = password</pre><p>
                                </p></li><li class="listitem"><p>Restart both Glance services:
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service glance-api restart &amp;&amp; service glance-registry restart</code></strong></pre><p>
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openstack-glance-api restart &amp;&amp; service openstack-glance-registry restart</code></strong></pre><p>
                                    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Check the <code class="literal">/var/log/glance/*.log</code> files for errors that would prevent
                                            the Image Service from successfully starting.</p></div><p>                                    
                                </p></li><li class="listitem"><p>Create Glance tables into the database:
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>glance-manage db_sync</code></strong></pre></li><li class="listitem"><p>Download and import Ubuntu 12.04 LTS UEC Image:
                            </p><pre class="screen"><code class="prompt"># </code><strong class="userinput"><code>wget http://uec-images.ubuntu.com/releases/12.04/release/ubuntu-12.04-server-cloudimg-amd64-disk1.img</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>glance image-create --is-public true --disk-format qcow2 --container-format bare --name "Ubuntu" &lt; ubuntu-12.04-server-cloudimg-amd64-disk1.img</code></strong>                     </pre><p>
                        </p><p>Download and import Cirros QCOW2 Image:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>wget http://download.cirros-cloud.net/0.3.1/cirros-0.3.1-x86_64-disk.img</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>glance image-create --is-public true --disk-format qcow2 --container-format bare --name "Cirros 0.3.1" &lt; cirros-0.3.1-x86_64-disk.img</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Check if the images have been introduced in the index:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>glance image-list</code></strong>
<strong class="userinput"><code>
+--------------------------------------+---------------------------------+-------------+------------------+-----------+--------+
| ID                                   | Name                            | Disk Format | Container Format | Size      | Status |
+--------------------------------------+---------------------------------+-------------+------------------+-----------+--------+
| acafc7c0-40aa-4026-9673-b879898e1fc2 | Cirros 0.3.1                    | qcow2       | bare             | 13147648  | active |
| 10ccdf86-e59e-41ac-ab41-65af91ea7a9c | cirros-0.3.0-x86_64-uec         | ami         | ami              | 25165824  | active |
| 8473f43f-cd1f-47cc-8d88-ccd9a62e566f | cirros-0.3.0-x86_64-uec-kernel  | aki         | aki              | 4731440   | active |
| 75c1bb27-a406-462c-a379-913e4e6221c9 | cirros-0.3.0-x86_64-uec-ramdisk | ari         | ari              | 2254249   | active |
| 62f9278e-a26e-4fa0-9537-1eb503aa2f01 | Ubuntu                          | qcow2       | bare             | 251985920 | active |
+--------------------------------------+---------------------------------+-------------+------------------+-----------+--------+</code></strong></pre></li></ul></div></li></ol></div><p>
            </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_controller-nova"></a>OpenStack Compute (Cloud Controller services)</h3></div></div></div><p>The OpenStack Compute Service provides the cloud environment with the ability to manage the scheduling,
        creation and deletion of virtual machines (instances).
                </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the Nova packages:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install nova-api nova-cert nova-common nova-conductor \
    nova-scheduler python-nova python-novaclient nova-consoleauth novnc \
    nova-novncproxy</code></strong></pre><p>
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openstack-nova-api openstack-nova-scheduler openstack-nova-cert \
        openstack-nova-console openstack-nova-doc genisoimage openstack-dashboard \
        openstack-nova-novncproxy openstack-nova-conductor novnc</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Configure Nova:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <code class="filename">/etc/nova/api-paste.ini</code>:
                                    </p><pre class="programlisting">admin_tenant_name = service 
admin_user = nova 
admin_password = password</pre><p>
                                    </p><pre class="programlisting">[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
service_protocol = http
service_host = 127.0.0.1
service_port = 5000
admin_tenant_name = service
admin_user  = nova
admin_password = password
# Workaround for https://bugs.launchpad.net/nova/+bug/1154809
auth_version = v2.0</pre><p>
</p></li><li class="listitem"><p>Add the following to the <span class="bold"><strong>/etc/nova/nova.conf</strong></span> file. This file is
                                    the main configuration file of Nova. There is a large amount of configuration options
                                    that can go in this file. This guide illustrates the minimum needed for a simple environment.
                                    Note that the nova.conf file supplied by your distribution will have some options already set. Leave
                                    them as-is.
                                    </p><pre class="programlisting">[DEFAULT]

sql_connection=mysql://nova:password@localhost/nova
rabbit_password=password
auth_strategy=keystone

# Networking
network_api_class=nova.network.quantumv2.api.API
quantum_url=http://10.10.10.10:9696
quantum_auth_strategy=keystone
quantum_admin_tenant_name=service
quantum_admin_username=quantum
quantum_admin_password=password
quantum_admin_auth_url=http://10.10.10.10:35357/v2.0
libvirt_vif_driver=nova.virt.libvirt.vif.LibvirtHybridOVSBridgeDriver
linuxnet_interface_driver=nova.network.linux_net.LinuxOVSInterfaceDriver  

# Security Groups                                    
firewall_driver=nova.virt.firewall.NoopFirewallDriver
security_group_api=quantum                           
                                                     
# Metadata                                           
quantum_metadata_proxy_shared_secret=password          
service_quantum_metadata_proxy=true                  
metadata_listen = 10.10.10.10        
metadata_listen_port = 8775                          

# Cinder
volume_api_class=nova.volume.cinder.API

# Glance
glance_api_servers=10.10.10.10:9292
image_service=nova.image.glance.GlanceImageService

# novnc
novnc_enable=true             
novncproxy_port=6080          
novncproxy_host=10.0.0.10
vncserver_listen=0.0.0.0      
</pre><p>
</p><pre class="programlisting">
# General
verbose = True
qpid_username=guest
qpid_password=guest
rpc_backend = nova.openstack.common.rpc.impl_qpid

# Networking
network_api_class=nova.network.quantumv2.api.API
quantum_url=http://10.10.10.10:9696
quantum_auth_strategy=keystone
quantum_admin_tenant_name=service
quantum_admin_username=quantum
quantum_admin_password=password
quantum_admin_auth_url=http://10.10.10.10:35357/v2.0
libvirt_vif_driver=nova.virt.libvirt.vif.LibvirtHybridOVSBridgeDriver
linuxnet_interface_driver=nova.network.linux_net.LinuxOVSInterfaceDriver  

# Security Groups                                    
firewall_driver=nova.virt.firewall.NoopFirewallDriver
security_group_api=quantum                           
                                                     
# Metadata                                           
quantum_metadata_proxy_shared_secret=password          
service_quantum_metadata_proxy=true                  
metadata_listen = 10.10.10.10        
metadata_listen_port = 8775                          

# Cinder
volume_api_class=nova.volume.cinder.API

# Glance
glance_api_servers=10.10.10.10:9292
image_service=nova.image.glance.GlanceImageService

# novnc
novnc_enable=true             
novncproxy_port=6080          
novncproxy_host=10.0.0.10
vncserver_listen=0.0.0.0      
</pre><p>
                                </p></li><li class="listitem"><p>Create Nova tables into the database:
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>nova-manage db sync</code></strong></pre></li><li class="listitem"><p>Restart Nova services:
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service nova-api restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service nova-cert restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service nova-consoleauth restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service nova-scheduler restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service nova-conductor restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service nova-novncproxy restart</code></strong></pre><p>
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openstack-nova-api restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service openstack-nova-cert restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service openstack-nova-consoleauth restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service openstack-nova-scheduler restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service openstack-nova-conductor restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service openstack-nova-novncproxy restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-nova-api on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-nova-cert on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-nova-consoleauth on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-nova-scheduler on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-nova-conductor on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-nova-novncproxy on</code></strong></pre><p>
                                    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Check the <code class="literal">/var/log/nova/nova-*</code> files for any errors
                                            that would prevent the Compute Service from successfully starting.</p></div><p>
                                </p></li></ul></div></li></ol></div><p>
            </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_controller-cinder"></a>OpenStack Block Storage</h3></div></div></div><p>While Cinder contains many different storage drivers, the
        most common and basic configuration uses LVM and iSCSI. This
        guide illustrates how to use one disk
            (<code class="literal">/dev/sdb</code>) in an LVM Volume Group
        called <code class="literal">cinder-volumes</code>. When a user requests
        a block storage volume, a Logical Volume is created from this
        Volume Group and then mounted on the user's instance by way of
        iSCSI. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the Cinder packages: </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install cinder-api cinder-scheduler cinder-volume iscsitarget \
    open-iscsi iscsitarget-dkms python-cinderclient linux-headers-`uname -r`</code></strong></pre><p>
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openstack-cinder openstack-cinder-doc \
        iscsi-initiator-utils scsi-target-utils</code></strong></pre><p>
                </p></li><li class="listitem"><p>Configure &amp; start the iSCSI services: </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>sed -i 's/false/true/g' /etc/default/iscsitarget</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service iscsitarget start</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service open-iscsi start</code></strong></pre><p>
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service tgtd start</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service iscsi start</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig tgtd on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig iscsi on</code></strong></pre></li><li class="listitem"><p>Configure Cinder:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <code class="filename">/etc/cinder/cinder.conf</code>:
                            </p><pre class="programlisting">[DEFAULT]
sql_connection = mysql://cinder:password@localhost/cinder
rabbit_password = password

[keystone_authtoken]
admin_tenant_name = service
admin_user = cinder 
admin_password = password
service_protocol = http
service_host = localhost
service_port = 5000
auth_host = localhost
auth_port = 35357
auth_protocol = http</pre><pre class="programlisting">[DEFAULT]
rpc_backend = cinder.openstack.common.rpc.impl_qpid
sql_connection = mysql://cinder:password@localhost/cinder
qpid_user = guest
qpid_password = quest

[keystone_authtoken]
admin_tenant_name = service
admin_user = cinder 
admin_password = password
service_protocol = http
service_host = localhost
service_port = 5000
auth_host = localhost
auth_port = 35357
auth_protocol = http</pre></li><li class="listitem"><p>Edit <code class="filename">/etc/cinder/api-paste.ini</code> in the
                                <code class="literal">filter_authtoken</code> section so it does not contain
                            any of the variables defined in the
                                <code class="literal">keystone_authtoken</code> section of
                                <code class="filename">cinder.conf</code>:
                            </p><pre class="programlisting">[filter:authtoken]
paste.filter_factory = keystone.middleware.auth_token:filter_factory
# This section should not contain any other config options</pre><p>
                        </p></li><li class="listitem"><p>Create the LVM Physical Volume and
                            Logical Volume:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>pvcreate /dev/sdb</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>vgcreate cinder-volumes /dev/sdb</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Create Cinder tables into the database:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>cinder-manage db sync</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Restart the services: </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service cinder-api restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service cinder-scheduler restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service cinder-volume restart</code></strong></pre><p>
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openstack-cinder-api restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service openstack-cinder-scheduler restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service openstack-cinder-volume restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-cinder-api on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-cinder-scheduler on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-cinder-volume on</code></strong></pre></li></ul></div></li></ol></div><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_controller-quantum"></a>OpenStack Network Service (Cloud Controller)</h3></div></div></div><p>The OpenStack Network Service provides a comprehensive and extendible networking service to the cloud. Some features include,
        but are not limited to, the ability for instances to reach an external network outside of the cloud as well as the ability
        for each user of the cloud to create multiple internal subnets of their own.
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the Quantum Server:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install quantum-server</code></strong></pre><p>
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openstack-quantum openstack-quantum-openvswitch</code></strong></pre><p>
                    </p></li><li class="listitem"><p>Configure the Quantum service:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/quantum.conf</strong></span>:
                                </p><pre class="programlisting">[DEFAULT]
verbose = True
rabbit_password = password
[keystone_authtoken]
admin_tenant_name = service
admin_user = quantum 
admin_password = password</pre><p>
                                </p><pre class="programlisting">[DEFAULT]
core_plugin = \
    quantum.plugins.openvswitch.ovs_quantum_plugin.OVSQuantumPluginV2
auth_strategy = keystone
fake_rabbit = False
rpc_backend=quantum.openstack.common.rpc.impl_qpid
qpid_username = guest
qpid_password = guest</pre><p>
                            </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/plugins/openvswitch/ovs_quantum_plugin.ini</strong></span>:
                                </p><pre class="programlisting">[DATABASE]
sql_connection = mysql://quantum:password@localhost/quantum
[OVS]
tenant_network_type = gre 
tunnel_id_ranges = 1:1000
enable_tunneling = True
local_ip = 10.10.10.10
[SECURITYGROUP]
firewall_driver = quantum.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</pre><p>
                                 </p><pre class="programlisting">[DATABASE]
sql_connection = mysql://quantum:password@localhost/quantum
[OVS]
enable_tunneling = False
[SECURITYGROUP]
firewall_driver = quantum.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>It's more handy to choose
                               <span class="bold"><strong>tunnel mode</strong></span> since you don't
                               have to configure your physical
                               switches for VLANs.</p><p>The Fedora kernel module for OpenVSwitch has been compiled without support for tunnels. To use gre tunnels the module will have to be recompiled.</p></div></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/api-paste.ini</strong></span>:
                                </p><pre class="programlisting">[filter:authtoken]
paste.filter_factory = keystoneclient.middleware.auth_token:filter_factory
auth_host = 127.0.0.1
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = quantum
admin_password = password
</pre></li></ul></div></li><li class="listitem"><p>Enable the OVS plugin:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>ln -s /etc/quantum/plugins/openvswitch/ovs_quantum_plugin.ini /etc/quantum/plugin.ini</code></strong></pre><p>
                    </p></li><li class="listitem"><p>Set SELinux to permissive mode:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>setenforce 0</code></strong>    
<code class="prompt">#</code> <strong class="userinput"><code>sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux</code></strong></pre><p>
                    </p></li><li class="listitem"><p>Start the services:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service quantum-server restart</code></strong></pre><p>
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service quantum-server restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig quantum-server on</code></strong></pre></li></ol></div><p>
        </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_controller-dashboard"></a>OpenStack Dashboard</h3></div></div></div><p>The OpenStack Dashboard service provides users of the cloud environment with a web-accessible GUI as an alternative
        to using the command-line tools.</p><p>To enable it, install the Horizon package and its dependencies:
        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install openstack-dashboard memcached python-memcache</code></strong></pre><p>
        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install httpd memcached
 ( cat | sudo tee -a /etc/openstack-dashboard/local_settings ) &lt;&lt;EOF
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'horizon',
        'USER': 'horizon',
        'PASSWORD': 'password',
        'HOST': '$MY_IP',
        'PORT': '',
    }
}
EOF</code></strong></pre><p>
        </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Optional, but recommended: remove the <code class="literal">openstack-dashboard-ubuntu-theme</code> package. 
                This theme prevents several menus as well as the network map from rendering correctly:
                </p><pre class="screen"><strong class="userinput"><code>apt-get remove --purge openstack-dashboard-ubuntu-theme</code></strong></pre><p>            
            </p></div><p>
        
    OpenStack Dashboard is now available at <span class="bold"><strong>http://cloud/horizon</strong></span>.
    We can login with the <span class="bold"><strong>admin</strong></span> / <span class="bold"><strong>password</strong></span> credentials
    or <span class="bold"><strong>demo</strong></span> / <span class="bold"><strong>password</strong></span>.
    
    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Check the <code class="literal">/var/log/apache/error.log</code> file for errors that wold prevent
        either the Apache service or the Dashboard service from successfully starting.</p></div><p>
        </p></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="basic-install_network"></a>Network Node</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_network-intro"></a>Introduction</h3></div></div></div><p>The Network node will provide:
        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Virtual Bridging (Open-vSwitch + Quantum Agent) with tunneling</p></li><li class="listitem"><p>DHCP Server (Quantum DHCP Agent)</p></li><li class="listitem"><p>Virtual Routing (Quantum L3 Agent)</p></li></ul></div><p>
        </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>It is entirely possible to install all of these services on the Cloud Controller.
            If you are short of resources, this is a good alternative.</p></div><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-network-common"></a>Common services</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="network-os"></a>Operating System</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install <span class="phrase">Fedora 18</span>
                    <span class="phrase">Ubuntu 12.04</span>. The exact installation procedure is outside the scope of this document, but please note the following configurations:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Time zone: <span class="bold"><strong>UTC</strong></span></p></li><li class="listitem"><p>Hostname: <span class="bold"><strong>network</strong></span></p></li><li class="listitem"><p>Packages: <span class="bold"><strong>OpenSSH-Server</strong></span></p></li></ul></div><p>Once installation has finished, the server will reboot.</p></li><li class="listitem"><p>Since the default OpenStack release
                    in Ubuntu 12.04 LTS is older, we are going to use
                    the Ubuntu Cloud Archive for Grizzly:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install ubuntu-cloud-keyring</code></strong></pre><p>
                    Edit <span class="bold"><strong>/etc/apt/sources.list.d/cloud-archive.list</strong></span>:
                        </p><pre class="programlisting">deb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/grizzly main</pre><p>
                    Upgrade the system (and reboot if needed):
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get update &amp;&amp; apt-get dist-upgrade</code></strong></pre><p>
                </p><p>Use the Fedora repositories for Grizzly:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>wget http://rdo.fedorapeople.org/openstack/openstack-grizzly/fedora-openstack-grizzly.repo</code></strong></pre><p>
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>mv fedora-openstack-grizzly.repo /etc/yum.repos.d/</code></strong></pre><p>
                </p></li><li class="listitem"><p>Configure the network:</p><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <span class="bold"><strong>/etc/network/interfaces</strong></span>:
                                </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>This will change later on in
                                   the guide when Open vSwitch is
                                   configured.</p></div><p>
                                </p><pre class="programlisting"># Internal Network
auto eth0
iface eth0 inet static
    address 10.10.10.9
    netmask 255.255.255.0

# External Network
auto eth1
iface eth1 inet static
    address 10.0.0.9
    netmask 255.255.255.0
    gateway 10.0.0.1
    dns-nameservers 8.8.8.8</pre><p>                                
                            </p></li></ul></div><p>
                </p><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysctl.conf</strong></span>:
                                </p><pre class="programlisting">net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0</pre><p>
                                Then, restart the network service:
                                </p><pre class="screen"><strong class="userinput"><code>service networking restart</code></strong></pre><p>                            
                            </p></li></ul></div><p>
                </p><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Set up old ethernet nic device names:</p><p>
                                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>sed -i 's/# GOTO="netdevicename_end"/GOTO="netdevicename_end"/g'/lib/udev/rules.d/71-biosdevname.rules</code></strong></pre><p>
                            </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysconf/network-scripts/ifcfg-eth0</strong></span>:
                                </p><pre class="programlisting"># Internal Network
DEVICE=eth0
TYPE=Ethernet
BOOTPROTO=static
IPADDR=10.10.10.9
NETMASK=255.255.255.0
GATEWAY=192.168.0.254
DNS1=8.8.8.8
DEFROUTE=yes
ONBOOT=yes
    </pre><p>                                                           
                            </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysconf/network-scripts/ifcfg-eth1</strong></span>:
                                </p><pre class="programlisting"># External
DEVICE=eth1
TYPE=Ethernet
BOOTPROTO=static
IPADDR=10.10.10.1
NETMASK=255.255.255.0
DEFROUTE=yes
ONBOOT=yes
</pre><p>
                            </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysconf/network-scripts/ifcfg-eth2</strong></span> file:
                                </p><pre class="programlisting">#Public Bridge
DEVICE=eth2
TYPE=Ethernet
BOOTPROTO=static
IPADDR=10.10.10.1
NETMASK=255.255.255.0
DEFROUTE=yes
ONBOOT=yes
</pre><p>
                                
                            </p></li><li class="listitem"><p>
                                Reboot.
                            </p></li></ul></div><p>
                </p><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit the <span class="bold"><strong>/etc/hosts</strong></span> file and add
                                    <span class="bold"><strong>controller</strong></span>, 
                                    <span class="bold"><strong>networknode</strong></span> and 
                                    <span class="bold"><strong>compute1</strong></span> hostnames with correct IP.
                                </p><pre class="programlisting">127.0.0.1       localhost
10.10.10.10     cloud
10.10.10.9      network
10.10.10.11     c01</pre><p>
                                
                            </p></li></ul></div><p>
                </p></li><li class="listitem"><p>Install NTP:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p> 
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install ntp</code></strong></pre><p>
                                    </p><pre class="screen"><code class="prompt">#</code><strong class="userinput"><code>yum install ntp</code></strong></pre><p>
                                </p></li></ul></div></li></ol></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_network-services"></a>OpenStack Networking (Network Controller)</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="network-ovs"></a>Open vSwitch</h4></div></div></div><p>
        </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the packages:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install quantum-plugin-openvswitch-agent \
quantum-dhcp-agent quantum-l3-agent</code></strong></pre><p>
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openstack-quantum openstack-quantum-openvswitch\
openvswitch-switch</code></strong></pre><p>The Open vSwitch kernel module in Fedora
                        has been compiled without tunnel support. If
                        gre tunnels and network namespaces are
                        desired, then this package must be recompiled
                        from source. Directions can be found on the
                        Open vSwitch site. If the kernel module is
                        recompiled then the existing one must be
                        removed and deleted.</p></li><li class="listitem"><p>Start Open vSwitch:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openvswitch-switch start</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openvswitch-switch on</code></strong></pre><p>
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openvswitch-switch start</code></strong></pre><p>
                </p></li><li class="listitem"><p>Create an internal and external network bridge. The purposes of these bridges are described in the Introduction of this guide.                     
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>ovs-vsctl add-br br-ex</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>ovs-vsctl add-port br-ex eth1</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>ovs-vsctl add-br br-int</code></strong>
</pre></li><li class="listitem"><p>Configure the bridges:
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Change the <code class="literal">eth1</code> entry in <span class="bold"><strong>/etc/network/interfaces</strong></span> to look like:
                                    </p><pre class="programlisting">auto eth1
iface eth1 inet manual              
    up ip address add 0/0 dev $IFACE
    up ip link set $IFACE up        
    down ip link set $IFACE down</pre><p>
                                </p></li><li class="listitem"><p>Add <code class="literal">br-ex</code> to <span class="bold"><strong>/etc/network/interfaces</strong></span>:
                                </p><pre class="programlisting">auto br-ex
iface br-ex inet static
    address 10.0.0.9
    netmask 255.255.255.0
    gateway 10.0.0.1
</pre></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysconf/network-scripts/ifcfg-eth1</strong></span>:
                                </p><pre class="programlisting"># External
DEVICE=eth1
TYPE=Ethernet
BOOTPROTO=none
NM_CONTROLLED=no
BRIDGE=br-ex
ONBOOT=yes
</pre><p>
                            </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysconf/network-scripts/ifcfg-br-ex</strong></span> file:
                                </p><pre class="programlisting">#Public Bridge
DEVICE=br-ex
TYPE=Bridge
BOOTPROTO=static
IPADDR=10.10.10.9
NETMASK=255.255.255.0
NM_CONTROLLED=no
ONBOOT=yes
</pre><p>
                                
                            </p></li><li class="listitem"><p>Remove the IP address from <code class="literal">eth1</code> add it to
                                        <code class="literal">br-ex</code>:
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>ip addr del 10.0.0.9/24 dev eth1</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>ip addr add 10.0.0.9/24 dev br-ex</code></strong></pre><p>
                                </p></li><li class="listitem"><p>Restart networking:
                                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service networking restart</code></strong></pre><p>
                            </p></li></ul></div><p>
                </p></li><li class="listitem"><p>Finally, enable a simple NAT service so that the Compute Node(s)
                        can access the Internet through the Cloud Controller:
                        </p><pre class="screen"><strong class="userinput"><code>echo 1 &gt; /proc/sys/net/ipv4/conf/all/forwarding
iptables -A FORWARD -i eth0 -o br-ex -s 10.10.10.0/24 -m conntrack --ctstate NEW -j ACCEPT
iptables -A FORWARD -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
iptables -A POSTROUTING -s 10.10.10.0/24 -t nat -j MASQUERADE</code></strong></pre></li></ol></div><p>
    </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="network-quantum"></a>Quantum</h4></div></div></div><p>Configure the Quantum services:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/quantum.conf</strong></span>:
                        </p><pre class="programlisting">[DEFAULT]
verbose = True
rabbit_password = password
rabbit_host = 10.10.10.10
[keystone_authtoken]
auth_host = 10.10.10.10
admin_tenant_name = service
admin_user = quantum 
admin_password = password</pre><p>
                        </p><pre class="programlisting">[DEFAULT]
verbose = True
rpc_backend = cinder.openstack.common.rpc.impl_qpid
sql_connection = mysql://cinder:password@localhost/cinder
qpid_user = guest
qpid_password = quest
[keystone_authtoken]
auth_host = 10.10.10.10
admin_tenant_name = service
admin_user = quantum 
admin_password = password</pre></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/plugins/openvswitch/ovs_quantum_plugin.ini</strong></span>:
                        </p><pre class="programlisting">[DATABASE]
sql_connection = mysql://quantum:password@10.10.10.10/quantum
[OVS]
tenant_network_type = gre 
tunnel_id_ranges = 1:1000
enable_tunneling = True
local_ip = 10.10.10.9
[SECURITYGROUP]                                                                        
firewall_driver = quantum.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver</pre><p>
                    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>It's more handy to choose <span class="bold"><strong>tunnel mode</strong></span>
                            since you don't have to configure your
                            physical switches for VLANs.</p></div></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/dhcp_agent.ini</strong></span>:
                        </p><pre class="programlisting">[DEFAULT]
enable_isolated_metadata = True
enable_metadata_network = True</pre></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/metadata_agent.ini</strong></span>:
                    </p><pre class="programlisting">[DEFAULT]
auth_url = http://10.10.10.10:35357/v2.0
auth_region = RegionOne               
admin_tenant_name = service           
admin_user = quantum                  
admin_password = password
nova_metadata_ip = 10.10.10.10
metadata_proxy_shared_secret = password
                    </pre></li></ul></div><p>Start the services:
                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service quantum-plugin-openvswitch-agent start</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service quantum-dhcp-agent restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service quantum-metadata-agent restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service quantum-l3-agent restart</code></strong></pre><p>
                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service quantum-server restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service quantum-openvswitch-agent restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service quantum-dhcp-agent restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>service quantum-l3-agent restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig quantum-server on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig quantum-openvswitch-agent on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig quantum-dhcp-agent on</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig quantum-l3-agent on</code></strong></pre><p>
            
            </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>
            Check the <code class="literal">/var/log/quantum/*.log</code> files for errors that would prevent
            the Networking Service from successfully starting.
        </p></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_network-operating"></a>Virtual Networking</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="create-networking"></a>Create Virtual Networking</h4></div></div></div><p>
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Create an <code class="literal">openrc</code> File</p><p>
                        </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Create a file called <span class="bold"><strong>~/openrc</strong></span>. This
                                    file contains the OpenStack admin credentials that will be used when interacting with the OpenStack 
                                    environment on the command line.
                                    </p><pre class="programlisting">export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=password
export OS_AUTH_URL="http://10.10.10.10:5000/v2.0/"
export SERVICE_ENDPOINT="http://10.10.10.10:35357/v2.0"
export SERVICE_TOKEN=password</pre></li></ul></div><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Source the credentials into your environment:
                                </p><pre class="screen"><strong class="userinput"><code>source ~/openrc</code></strong></pre></li><li class="listitem"><p>
                                Configure the Bash shell to load these credentials upon each login:
                                </p><pre class="screen"><strong class="userinput"><code>echo "source ~/openrc" &gt;&gt; ~/.bashrc</code></strong></pre><p>
                            </p></li></ul></div></li><li class="listitem"><p>The following bash script will create an internal network for the "demo" project. 
                        </p><pre class="programlisting">#!/bin/bash
TENANT_NAME="demo"
TENANT_NETWORK_NAME="demo-net"
TENANT_SUBNET_NAME="${TENANT_NETWORK_NAME}-subnet"
TENANT_ROUTER_NAME="demo-router"
FIXED_RANGE="10.5.5.0/24"
NETWORK_GATEWAY="10.5.5.1"
TENANT_ID=$(keystone tenant-list | grep " $TENANT_NAME " | awk '{print $2}')

TENANT_NET_ID=$(quantum net-create --tenant_id $TENANT_ID $TENANT_NETWORK_NAME --provider:network_type gre  --provider:segmentation_id 1 | grep " id " | awk '{print $4}')
TENANT_SUBNET_ID=$(quantum subnet-create --tenant_id $TENANT_ID --ip_version 4 --name $TENANT_SUBNET_NAME $TENANT_NET_ID $FIXED_RANGE --gateway $NETWORK_GATEWAY --dns_nameservers list=true 8.8.8.8 | grep " id " | awk '{print $4}')
ROUTER_ID=$(quantum router-create --tenant_id $TENANT_ID $TENANT_ROUTER_NAME | grep " id " | awk '{print $4}')
quantum router-interface-add $ROUTER_ID $TENANT_SUBNET_ID</pre></li></ol></div><p>
        </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="configure-l3"></a>L3 Configuration</h4></div></div></div><p>The Quantum L3 service enables instances to have external network access. If this service is not configured, your instances
            will only be able to communicate with each other. Please note that this configuration is highly dependant on your environment.
            For example, make note of the <code class="literal">subnet-create</code> command below. You will need to verify your own network settings
            for the external subnet (<code class="literal">10.0.0.0/24</code> in this case) as well as an allocation pool. The allocation pool
            is used to provide each Project with an IP address to access the external network. The pool consists of 50 IPs and therefore
            only 50 projects will be able to get a gateway IP.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Create an external network:
                    </p><pre class="screen"><strong class="userinput"><code>quantum net-create public --router:external=True</code></strong></pre><p>
                </p></li><li class="listitem"><p>Create a subnet for the external network:
                    </p><pre class="screen"><strong class="userinput"><code>quantum subnet-create --ip_version 4 --gateway 10.0.0.1 public 10.0.0.0/24 --allocation-pool start=10.0.0.200,end=10.0.0.250 --disable-dhcp --name public-subnet</code></strong></pre><p>
                </p></li><li class="listitem"><p>Set the gateway of the demo router to the public network:
                    </p><pre class="screen"><strong class="userinput"><code>quantum router-gateway-set demo-router public</code></strong></pre><p>
                </p></li></ul></div></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="basic-install_compute"></a>Compute Node</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_compute-intro"></a>Introduction</h3></div></div></div><p>The Compute node will provide :
	</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Hypervisor (KVM)</p></li><li class="listitem"><p>nova-compute</p></li><li class="listitem"><p>Quantum OVS Agent</p></li></ul></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_compute-common"></a>Common services</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="compute-os"></a>Operating System</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install <span class="phrase">Fedora 18</span>
                    <span class="phrase">Ubuntu 12.04</span>. Just like with the Cloud Controller, the exact steps are outside the
                    scope of this document, but please note the following options:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Time zone: <span class="bold"><strong>UTC</strong></span></p></li><li class="listitem"><p>Hostname: <span class="bold"><strong>c01</strong></span></p></li><li class="listitem"><p>Packages: <span class="bold"><strong>OpenSSH-Server</strong></span></p></li></ul></div><p>Once installation has finished, the server will reboot.</p></li><li class="listitem"><p>Since the default OpenStack release in Ubuntu 12.04 LTS is older,
                    we are going to use the Ubuntu Cloud Archive for Grizzly:
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install ubuntu-cloud-keyring</code></strong></pre><p>
                    Edit <span class="bold"><strong>/etc/apt/sources.list.d/cloud-archive.list</strong></span>:
                        </p><pre class="programlisting">deb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/grizzly main</pre><p>
                    Upgrade the system (and reboot if you need):
                        </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>sudo apt-get update &amp;&amp; apt-get upgrade</code></strong></pre><p>
                </p><p>Use the Fedora repositories for Grizzly:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>wget http://repos.fedorapeople.org/repos/openstack/openstack-grizzly/fedora-openstack-grizzly.repo</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>yum install http://repos.fedorapeople.org/repos/openstack/openstack-grizzly/rdo-release-grizzly-1.noarch.rpm </code></strong></pre><p>
                </p></li><li class="listitem"><p>Configure the network:</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>This will change later on in the guide
                        when Open vSwitch is configured</p></div><p>
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <span class="bold"><strong>/etc/network/interfaces</strong></span>:</p><pre class="programlisting"># Internal Network
auto eth0
    iface eth0 inet static
    address 10.10.10.11
    netmask 255.255.255.0
    gateway 10.10.10.9
    dns-nameservers 8.8.8.8
</pre></li></ul></div><p>                    
                </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <span class="bold"><strong>/etc/sysctl.conf</strong></span>:
                            </p><pre class="programlisting">net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0</pre><p>
                                Then, restart the network service:
                                </p><pre class="screen"><strong class="userinput"><code>service networking restart</code></strong></pre><p>
                            </p></li></ul></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Set up old ethernet nic device names:</p><p>
                                </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>sed -i 's/# GOTO="netdevicename_end"/GOTO="netdevicename_end"/g'/lib/udev/rules.d/71-biosdevname.rules</code></strong></pre><p>
                            </p><p>Edit <span class="bold"><strong>/etc/sysconf/network-scripts/ifcfg-eth0</strong></span>:</p><pre class="programlisting"># Internal Network
DEVICE=eth0
TYPE=Ethernet
BOOTPROTO=static
IPADDR=10.10.10.11
NETMASK=255.255.255.0
GATEWAY=10.10.10.9
DNS1=8.8.8.8
DEFROUTE=yes
ONBOOT=yes
    </pre></li><li class="listitem"><p>
                                Reboot.
                            </p></li></ul></div><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit the <span class="bold"><strong>/etc/hosts</strong></span> file and add
                                    <span class="bold"><strong>cloud</strong></span>, 
                                    <span class="bold"><strong>network</strong></span> and 
                                    <span class="bold"><strong>c01</strong></span> hostnames with correct IP.
                    </p><pre class="programlisting">127.0.0.1       localhost
10.10.10.10     cloud
10.10.10.9      network
10.10.10.11     c01</pre><p>
                </p></li></ul></div></li><li class="listitem"><p>Install NTP:
                    </p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p> 
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install ntp</code></strong></pre><p>
                                    </p><pre class="screen"><code class="prompt">#</code><strong class="userinput"><code>yum install ntp</code></strong></pre><p>
                                </p></li></ul></div></li></ol></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_compute-nova"></a>OpenStack Compute (Compute Node services)</h3></div></div></div><p>Just like with the Cloud Controller, the OpenStack Compute service is installed on the Compute Node. However,
        this time the <code class="literal">nova-compute</code> service is installed. This provides the Compute Node the capability
        to host virtual machines.
        </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the Nova Compute package:
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install nova-compute-kvm</code></strong></pre><p>
                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openstack-nova-compute</code></strong></pre><p>
                    </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p><code class="literal">nova-compute-kvm</code>
                            requires that your CPU supports
                            hardware-assisted virtualization (HVM)
                            such as Intel VT-x or AMD-V. If your CPU
                            does not support this, or if you are
                            already running in a virtualized
                            environment, you can instead use the
                                <code class="literal">nova-compute-qemu</code>
                            package. This package provides
                            software-based virtualization.</p></div><p>                            
                </p></li><li class="listitem"><p>Configure Nova:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Edit <span class="bold"><strong>/etc/nova/api-paste.ini</strong></span>:
                                    </p><pre class="programlisting">[filter:authtoken]
auth_host = 10.10.10.10
admin_tenant_name = service 
admin_user = nova 
admin_password = password</pre><p>
                                </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/nova/nova.conf</strong></span>:
                                    </p><pre class="programlisting">[DEFAULT]

# General
verbose=True
rabbit_host=10.10.10.10
rabbit_password=password

auth_strategy=keystone
ec2_host=10.10.10.10
ec2_url=http://10.10.10.10:8773/services/Cloud

# Networking
libvirt_use_virtio_for_bridges=True
network_api_class=nova.network.quantumv2.api.API
quantum_url=http://10.10.10.10:9696
quantum_auth_strategy=keystone
quantum_admin_tenant_name=service
quantum_admin_username=quantum
quantum_admin_password=password
quantum_admin_auth_url=http://10.10.10.10:35357/v2.0

# Security Groups                                    
firewall_driver=nova.virt.firewall.NoopFirewallDriver
security_group_api=quantum                           

# Compute #
compute_driver=libvirt.LibvirtDriver
connection_type=libvirt 

# Cinder
volume_api_class=nova.volume.cinder.API

# Glance
glance_api_servers=10.10.10.10:9292
image_service=nova.image.glance.GlanceImageService

# novnc
vnc_enabled=true                                            
vncserver_proxyclient_address=10.10.10.11                  
novncproxy_base_url=http://10.0.0.10:6080/vnc_auto.html
vncserver_listen=0.0.0.0</pre><p>
                                    </p><pre class="programlisting">[DEFAULT]

# General
verbose=True
qpid_hostname=192.168.206.130

auth_strategy=keystone
ec2_host=10.10.10.10
ec2_url=http://10.10.10.10:8773/services/Cloud

# Networking
libvirt_use_virtio_for_bridges=True
network_api_class=nova.network.quantumv2.api.API
quantum_url=http://10.10.10.10:9696
quantum_auth_strategy=keystone
quantum_admin_tenant_name=service
quantum_admin_username=quantum
quantum_admin_password=password
quantum_admin_auth_url=http://10.10.10.10:35357/v2.0

# Security Groups                                    
firewall_driver=nova.virt.firewall.NoopFirewallDriver
security_group_api=quantum                           

# Compute #
compute_driver=libvirt.LibvirtDriver
connection_type=libvirt 

# Cinder
volume_api_class=nova.volume.cinder.API

# Glance
glance_api_servers=10.10.10.10:9292
image_service=nova.image.glance.GlanceImageService

# novnc
vnc_enabled=true                                            
vncserver_proxyclient_address=10.10.10.11                  
novncproxy_base_url=http://10.0.0.10:6080/vnc_auto.html
vncserver_listen=0.0.0.0</pre></li><li class="listitem"><p>Restart Nova services:
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service nova-compute restart</code></strong></pre><p>
                                    </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openstack-nova-compute restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openstack-nova-compute on</code></strong></pre><p>
                                </p></li></ul></div></li></ol></div><p>
            </p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a name="basic-install_compute-quantum"></a>OpenStack Networking (Compute Node)</h3></div></div></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="compute-ovs"></a>Open vSwitch</h4></div></div></div><p>
                </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the packages:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install openvswitch-switch</code></strong></pre><p>
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openvswitch-switch</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Start Open vSwitch service
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openvswitch-switch start</code></strong></pre><p>
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service openvswitch-switch start</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig openvswitch-switch on</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Create an internal bridge. Just as described in the Introduction to this guide, the Compute Node
                            does not provide an external bridge. This enforces all instances' network traffic to go through
                            the Network Controller. This is known as a "single-node" networking setup. 
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>ovs-vsctl add-br br-int</code></strong></pre><p>
                        </p></li></ol></div><p>
            </p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a name="compute-quantum"></a>Quantum</h4></div></div></div><p>
                </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Install the packages:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>apt-get install quantum-plugin-openvswitch-agent</code></strong></pre><p>
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>yum install openstack-quantum-openvswitch</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/quantum.conf</strong></span>:
                            </p><pre class="programlisting">[DEFAULT]
rabbit_host = 10.10.10.10
rabbit_password = password
verbose = True</pre><p>
                            </p><pre class="programlisting">verbose = True
rpc_backend=quantum.openstack.common.rpc.impl_qpid</pre><p>
                        </p></li><li class="listitem"><p>Edit <span class="bold"><strong>/etc/quantum/plugins/openvswitch/ovs_quantum_plugin.ini</strong></span>:
                            </p><pre class="programlisting">[DATABASE]
sql_connection = mysql://quantum:password@10.10.10.1/quantum
[OVS]
tenant_network_type = gre
tunnel_id_ranges = 1:1000
local_ip = 10.10.10.11
enable_tunneling = True
[SECURITYGROUP]
firewall_driver = quantum.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
                            </pre><p>
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>ln -s /etc/quantum/plugins/openvswitch/ovs_quantum_plugin.ini /etc/quantum/plugin.ini</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Start the Agent:
                            </p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service quantum-plugin-openvswitch-agent restart</code></strong></pre><p>
                            </p><pre class="screen"><strong class="userinput"><code>service quantum-openvswitch-agent restart</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>chkconfig quantum-openvswitch-agent on</code></strong></pre><p>
                        </p></li><li class="listitem"><p>Ensure the cleanup utility is started on future boots:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>chkconfig quantum-ovs-cleanup on</code></strong></pre></li></ol></div><p>
                </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>Check the <code class="literal">/var/log/quantum/openvswitch-agent.log</code> file for errors that would
                    prevent the Networking service from successfully starting.</p></div><p>
                
            </p></div></div></div></div></div></body></html>
